{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13af748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical CPUs: 128\n",
      "Number of logical CPUs: 128\n",
      "Number of GPUs: 2\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "\tMemory Total: 81920.0 MB\n",
      "\tMemory Used: 7.0 MB\n",
      "\tMemory Free: 81042.0 MB\n",
      "\tGPU Utilization: 0.0%\n",
      "\tGPU Temperature: 31.0 Â°C\n",
      "GPU 2: NVIDIA A100 80GB PCIe\n",
      "\tMemory Total: 81920.0 MB\n",
      "\tMemory Used: 7.0 MB\n",
      "\tMemory Free: 81042.0 MB\n",
      "\tGPU Utilization: 0.0%\n",
      "\tGPU Temperature: 27.0 Â°C\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# GPU and CPU Check Code\n",
    "# KEEP AT THE TOP\n",
    "############################\n",
    "\n",
    "# !pip install psutil\n",
    "# !pip install gputil\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForMaskedLM, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Get the number of CPUs\n",
    "num_cpus = psutil.cpu_count(logical=False)  # physical cores\n",
    "num_logical_cpus = psutil.cpu_count(logical=True)  # logical cores\n",
    "\n",
    "print(f\"Number of physical CPUs: {num_cpus}\")\n",
    "print(f\"Number of logical CPUs: {num_logical_cpus}\")\n",
    "\n",
    "try:\n",
    "    import GPUtil\n",
    "\n",
    "    # Get the number of available GPUs\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    num_gpus = len(gpus)\n",
    "\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i + 1}: {gpu.name}\")\n",
    "        print(f\"\\tMemory Total: {gpu.memoryTotal} MB\")\n",
    "        print(f\"\\tMemory Used: {gpu.memoryUsed} MB\")\n",
    "        print(f\"\\tMemory Free: {gpu.memoryFree} MB\")\n",
    "        print(f\"\\tGPU Utilization: {gpu.load * 100}%\")\n",
    "        print(f\"\\tGPU Temperature: {gpu.temperature} Â°C\")\n",
    "except ImportError:\n",
    "    print(\"GPUtil library not found. Cannot check GPU information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720e1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE ONLY TO EXTRACT FILES FROM TAR FILES\n",
    "\n",
    "import tarfile\n",
    "\n",
    "def extract_all_files(tar_file_path, extract_to):\n",
    "    with tarfile.open(tar_file_path, 'r') as tar:\n",
    "        tar.extractall(extract_to)\n",
    "\n",
    "# Example usage\n",
    "tar_file_path = 'datasets/yahoo_answers_csv.tar.gz'\n",
    "extract_to = 'datasets/'\n",
    "extract_all_files(tar_file_path, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c6b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [01:23<00:00, 299.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Function to read texts from files within a folder\n",
    "def read_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read().strip())\n",
    "    return texts\n",
    "\n",
    "# Function to preprocess texts\n",
    "def preprocess(texts, tokenizer, max_length=512):\n",
    "    all_input_ids = []\n",
    "    for text in tqdm(texts):\n",
    "        # Tokenize using the provided tokenizer\n",
    "        tokenized = tokenizer.encode_plus(text, max_length=max_length, truncation=True, padding='max_length')\n",
    "        input_ids = tokenized['input_ids']\n",
    "        all_input_ids.append(input_ids)\n",
    "\n",
    "    return all_input_ids\n",
    "\n",
    "# Paths to the directories within aclImdb folder\n",
    "aclImdb_folder = \"datasets/aclImdb\"\n",
    "train_pos_path = os.path.join(aclImdb_folder, 'train', 'pos')\n",
    "train_neg_path = os.path.join(aclImdb_folder, 'train', 'neg')\n",
    "\n",
    "# Initialize the BERT-base-uncased tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Read and preprocess the texts from positive and negative folders\n",
    "train_pos_texts = read_texts_from_folder(train_pos_path)\n",
    "train_neg_texts = read_texts_from_folder(train_neg_path)\n",
    "train_texts = train_pos_texts + train_neg_texts\n",
    "\n",
    "processed = preprocess(train_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f89ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDITING FILE\n",
      "DONE EDITING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/vmenon19/Conda_Env/nlp2023v2/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTED TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/vmenon19/Conda_Env/nlp2023v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/data/user/vmenon19/Conda_Env/nlp2023v2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "print(\"EDITING FILE\")\n",
    "\n",
    "pretrain_file_path = \"pretraining_text.txt\"\n",
    "with open(pretrain_file_path, 'w', encoding='utf-8') as pretrain_file:\n",
    "    for text_ids in processed:\n",
    "        text = tokenizer.decode(text_ids, skip_special_tokens=True)\n",
    "        pretrain_file.write(text + '\\n')\n",
    "\n",
    "print(\"DONE EDITING\")\n",
    "        \n",
    "# Create a dataset for pre-training\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=pretrain_file_path,\n",
    "    block_size=512  # Adjust the block size as per your sequence length\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Probability of masking tokens\n",
    ")\n",
    "\n",
    "# Initialize the BERT masked language model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrained_bert_imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=10000\n",
    ")\n",
    "\n",
    "# Create Trainer instance for pre-training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"STARTED TRAINING\")\n",
    "\n",
    "# Start pre-training\n",
    "trainer.train()\n",
    "\n",
    "print(\"TRAINING DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e4357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
