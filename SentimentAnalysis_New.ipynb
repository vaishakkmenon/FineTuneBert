{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b80777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTS DONE\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# GPU and CPU Check Code\n",
    "# KEEP AT THE TOP\n",
    "############################\n",
    "\n",
    "# !pip install psutil\n",
    "# !pip install gputil\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import spacy\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "from functools import partial\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "\n",
    "print(\"IMPORTS DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773685b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical CPUs: 128\n",
      "Number of logical CPUs: 128\n",
      "Number of GPUs: 2\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "\tUUID: GPU-c9f222a4-cdf9-a690-b27c-fd6ca8dd8832\n",
      "\tMemory Total: 81920.0 MB\n",
      "\tMemory Used: 7.0 MB\n",
      "\tMemory Free: 81042.0 MB\n",
      "\tGPU Utilization: 0.0%\n",
      "\tGPU Temperature: 31.0 °C\n",
      "GPU 2: NVIDIA A100 80GB PCIe\n",
      "\tUUID: GPU-051052d6-9db8-7e01-b26b-69a92eab9f9e\n",
      "\tMemory Total: 81920.0 MB\n",
      "\tMemory Used: 7.0 MB\n",
      "\tMemory Free: 81042.0 MB\n",
      "\tGPU Utilization: 0.0%\n",
      "\tGPU Temperature: 28.0 °C\n"
     ]
    }
   ],
   "source": [
    "# Get the number of CPUs\n",
    "num_cpus = psutil.cpu_count(logical=False)  # physical cores\n",
    "num_logical_cpus = psutil.cpu_count(logical=True)  # logical cores\n",
    "\n",
    "print(f\"Number of physical CPUs: {num_cpus}\")\n",
    "print(f\"Number of logical CPUs: {num_logical_cpus}\")\n",
    "\n",
    "try:\n",
    "    import GPUtil\n",
    "\n",
    "    # Get the number of available GPUs\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    num_gpus = len(gpus)\n",
    "\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i + 1}: {gpu.name}\")\n",
    "        print(f\"\\tUUID: {gpu.uuid}\")\n",
    "        print(f\"\\tMemory Total: {gpu.memoryTotal} MB\")\n",
    "        print(f\"\\tMemory Used: {gpu.memoryUsed} MB\")\n",
    "        print(f\"\\tMemory Free: {gpu.memoryFree} MB\")\n",
    "        print(f\"\\tGPU Utilization: {gpu.load * 100}%\")\n",
    "        print(f\"\\tGPU Temperature: {gpu.temperature} °C\")\n",
    "except ImportError:\n",
    "    print(\"GPUtil library not found. Cannot check GPU information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb38bcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted data sample:\n",
      "{'input_ids': tensor([[  101,  1045,  3866,  2146,  2126,  2461,  1998,  2347,  1005,  1056,\n",
      "          2130,  5204,  1997,  2679,  2000,  4830,  6673,  2127,  1045,  2387,\n",
      "          2009,  2006,  1996, 15475,  1997,  2026,  2334, 17006,  1012,  1045,\n",
      "          4149,  2009,  1998,  2044,  1037,  3621,  1005, 17012,  2097,  2023,\n",
      "          2022,  2004,  2204,  1005,  2034,  2792,  1045,  2787,  2008,  2009,\n",
      "          2001,  1012,  4918, 22017, 14515,  2001,  2307,  2004,  2020,  1996,\n",
      "          2060,  2372,  1997,  1996,  3626,  1012,  2307,  2000,  2156,  2032,\n",
      "          2007,  1041,  7447,  2153,  1012,  2045,  2001,  1037,  4189,  2978,\n",
      "          1997, 25082,  1999,  2009,  2021,  2008,  2134,  1005,  1056,  8572,\n",
      "          2033,  1012,  2004,  2005,  2037,  2108,  2053,  5254,  1997,  2009,\n",
      "          2006,  1996,  7427,  1012,  2008,  2015,  2062,  2000,  2079,  2007,\n",
      "          1996, 10021,  4654,  3401, 27718,  2013,  5579,  8196,  2008,  1996,\n",
      "         22861, 11329,  2031,  1012,  2027,  2323,  2031,  2445,  2009,  1037,\n",
      "          2321,  2074,  2005,  1996,  2653,  2894,  1012,  1026,  7987,  1013,\n",
      "          1028,  1026,  7987,  1013,  1028,  3811,  6749,  2186,  1010,  1045,\n",
      "          2215,  2062,   999,   999,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "Number of training samples: 25000\n",
      "Number of testing samples: 25000\n",
      "Number of formatted data samples: 50000\n"
     ]
    }
   ],
   "source": [
    "# Directory paths\n",
    "train_pos_dir = 'datasets/aclImdb/train/pos/'\n",
    "train_neg_dir = 'datasets/aclImdb/train/neg/'\n",
    "test_pos_dir = 'datasets/aclImdb/test/pos/'\n",
    "test_neg_dir = 'datasets/aclImdb/test/neg/'\n",
    "\n",
    "# Function to read text files from a directory and assign labels based on directory structure\n",
    "def read_text_files_from_directory(directory):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    # For positive reviews (assuming 'pos' directory corresponds to label 1)\n",
    "    if 'pos' in directory:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0  # For negative reviews\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read().replace('\\n', ' ')\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "# Read positive and negative training data with labels\n",
    "train_pos_texts, train_pos_labels = read_text_files_from_directory(train_pos_dir)\n",
    "train_neg_texts, train_neg_labels = read_text_files_from_directory(train_neg_dir)\n",
    "\n",
    "# Read positive and negative test data with labels\n",
    "test_pos_texts, test_pos_labels = read_text_files_from_directory(test_pos_dir)\n",
    "test_neg_texts, test_neg_labels = read_text_files_from_directory(test_neg_dir)\n",
    "\n",
    "# Combine texts and labels\n",
    "train_texts = train_pos_texts + train_neg_texts\n",
    "train_labels = train_pos_labels + train_neg_labels\n",
    "test_texts = test_pos_texts + test_neg_texts\n",
    "test_labels = test_pos_labels + test_neg_labels\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Process the data for BERT input\n",
    "formatted_data = []\n",
    "max_seq_length = 512  # Maximum sequence length for BERT\n",
    "\n",
    "# Tokenize and format data\n",
    "for text in train_texts + test_texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    token_type_ids = encoded_dict['token_type_ids']\n",
    "\n",
    "    formatted_data.append({\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "    })\n",
    "\n",
    "# Display the formatted data sample\n",
    "print(\"Formatted data sample:\")\n",
    "for i in range(1):\n",
    "    print(formatted_data[i])\n",
    "\n",
    "# Check the lengths of labels and formatted data\n",
    "print(f\"Number of training samples: {len(train_labels)}\")\n",
    "print(f\"Number of testing samples: {len(test_labels)}\")\n",
    "print(f\"Number of formatted data samples: {len(formatted_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2832c293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n",
      "Processed batch 250/1875, Loss: 0.1830\n",
      "Processed batch 500/1875, Loss: 0.3208\n",
      "Processed batch 750/1875, Loss: 0.2559\n",
      "Processed batch 1000/1875, Loss: 0.1775\n",
      "Processed batch 1250/1875, Loss: 0.2661\n",
      "Processed batch 1500/1875, Loss: 0.1525\n",
      "Processed batch 1750/1875, Loss: 0.1152\n",
      "Train Loss: 0.2053\n",
      "Epoch Training Time: 1040.72 seconds\n",
      "Validation Loss: 0.1506, Validation Accuracy: 94.48%\n",
      "Full Epoch Time: 1076.97 seconds\n",
      "\n",
      "Epoch 2/4\n",
      "Processed batch 250/1875, Loss: 0.2292\n",
      "Processed batch 500/1875, Loss: 0.0455\n",
      "Processed batch 750/1875, Loss: 0.0326\n",
      "Processed batch 1000/1875, Loss: 0.2025\n",
      "Processed batch 1250/1875, Loss: 0.0547\n",
      "Processed batch 1500/1875, Loss: 0.1200\n",
      "Processed batch 1750/1875, Loss: 0.6036\n",
      "Train Loss: 0.1100\n",
      "Epoch Training Time: 1039.21 seconds\n",
      "Validation Loss: 0.1653, Validation Accuracy: 94.30%\n",
      "Full Epoch Time: 1074.79 seconds\n",
      "\n",
      "Epoch 3/4\n",
      "Processed batch 250/1875, Loss: 0.0105\n",
      "Processed batch 500/1875, Loss: 0.0047\n",
      "Processed batch 750/1875, Loss: 0.0079\n",
      "Processed batch 1000/1875, Loss: 0.0037\n",
      "Processed batch 1250/1875, Loss: 0.0273\n",
      "Processed batch 1500/1875, Loss: 0.0186\n",
      "Processed batch 1750/1875, Loss: 0.0774\n",
      "Train Loss: 0.0589\n",
      "Epoch Training Time: 1040.02 seconds\n",
      "Validation Loss: 0.1788, Validation Accuracy: 94.20%\n",
      "Full Epoch Time: 1075.59 seconds\n",
      "\n",
      "Epoch 4/4\n",
      "Processed batch 250/1875, Loss: 0.0176\n",
      "Processed batch 500/1875, Loss: 0.0006\n",
      "Processed batch 750/1875, Loss: 0.0062\n",
      "Processed batch 1000/1875, Loss: 0.0101\n",
      "Processed batch 1250/1875, Loss: 0.0044\n",
      "Processed batch 1500/1875, Loss: 0.0190\n",
      "Processed batch 1750/1875, Loss: 0.0028\n",
      "Train Loss: 0.0353\n",
      "Epoch Training Time: 1040.06 seconds\n",
      "Validation Loss: 0.2013, Validation Accuracy: 94.62%\n",
      "Full Epoch Time: 1075.99 seconds\n",
      "Best validation accuracy of 94.62 achieved at epoch 4\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code remains unchanged)\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for potential debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_data, val_data, train_labels_raw, val_labels_raw = train_test_split(\n",
    "    formatted_data, combined_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_inputs = torch.cat([example['input_ids'] for example in train_data], dim=0)\n",
    "train_masks = torch.cat([example['attention_mask'] for example in train_data], dim=0)\n",
    "train_token_types = torch.cat([example['token_type_ids'] for example in train_data], dim=0)\n",
    "\n",
    "val_inputs = torch.cat([example['input_ids'] for example in val_data], dim=0)\n",
    "val_masks = torch.cat([example['attention_mask'] for example in val_data], dim=0)\n",
    "val_token_types = torch.cat([example['token_type_ids'] for example in val_data], dim=0)\n",
    "\n",
    "# Define batch size and create DataLoader\n",
    "batch_size = 24\n",
    "\n",
    "train_labels = torch.tensor(train_labels_raw)\n",
    "val_labels = torch.tensor(val_labels_raw)\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_token_types, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_token_types, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the BERT model and tokenizer - Use BERT-Base or BERT-Large\n",
    "model_name = 'bert-base-uncased'  # For BERT-Base\n",
    "num_labels = 2  # Considering binary classification (positive/negative sentiment)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 4 # You can adjust the number of epochs\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "# Training and Validation loop with epoch timing\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    start_time = time.time()  # Start time for epoch\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, masks, token_types, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, attention_mask=masks, token_type_ids=token_types)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 250 == 0:\n",
    "            print(f\"Processed batch {batch_idx+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    end_time = time.time()  # End time for epoch\n",
    "    epoch_time = end_time - start_time  # Calculate epoch time\n",
    "    print(f\"Epoch Training Time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs, masks, token_types, labels = batch\n",
    "\n",
    "            outputs = model(inputs, attention_mask=masks, token_type_ids=token_types)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / len(val_dataset)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    end_time = time.time()  # End time for epoch\n",
    "    epoch_time = end_time - start_time  # Calculate epoch time\n",
    "    print(f\"Full Epoch Time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "print(f\"Best validation accuracy of {best_val_accuracy*100:.2f} achieved at epoch {best_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c069b8d",
   "metadata": {},
   "source": [
    "SAMPLE OUTPUT\n",
    "\n",
    "Epoch 1/4\n",
    "Processed batch 250/1875, Loss: 0.1830\n",
    "Processed batch 500/1875, Loss: 0.3208\n",
    "Processed batch 750/1875, Loss: 0.2559\n",
    "Processed batch 1000/1875, Loss: 0.1775\n",
    "Processed batch 1250/1875, Loss: 0.2661\n",
    "Processed batch 1500/1875, Loss: 0.1525\n",
    "Processed batch 1750/1875, Loss: 0.1152\n",
    "Train Loss: 0.2053\n",
    "Epoch Training Time: 1040.72 seconds\n",
    "Validation Loss: 0.1506, Validation Accuracy: 94.48%\n",
    "Full Epoch Time: 1076.97 seconds\n",
    "\n",
    "Epoch 2/4\n",
    "Processed batch 250/1875, Loss: 0.2292\n",
    "Processed batch 500/1875, Loss: 0.0455\n",
    "Processed batch 750/1875, Loss: 0.0326\n",
    "Processed batch 1000/1875, Loss: 0.2025\n",
    "Processed batch 1250/1875, Loss: 0.0547\n",
    "Processed batch 1500/1875, Loss: 0.1200\n",
    "Processed batch 1750/1875, Loss: 0.6036\n",
    "Train Loss: 0.1100\n",
    "Epoch Training Time: 1039.21 seconds\n",
    "Validation Loss: 0.1653, Validation Accuracy: 94.30%\n",
    "Full Epoch Time: 1074.79 seconds\n",
    "\n",
    "Epoch 3/4\n",
    "Processed batch 250/1875, Loss: 0.0105\n",
    "Processed batch 500/1875, Loss: 0.0047\n",
    "Processed batch 750/1875, Loss: 0.0079\n",
    "Processed batch 1000/1875, Loss: 0.0037\n",
    "Processed batch 1250/1875, Loss: 0.0273\n",
    "Processed batch 1500/1875, Loss: 0.0186\n",
    "Processed batch 1750/1875, Loss: 0.0774\n",
    "Train Loss: 0.0589\n",
    "Epoch Training Time: 1040.02 seconds\n",
    "Validation Loss: 0.1788, Validation Accuracy: 94.20%\n",
    "Full Epoch Time: 1075.59 seconds\n",
    "\n",
    "Epoch 4/4\n",
    "Processed batch 250/1875, Loss: 0.0176\n",
    "Processed batch 500/1875, Loss: 0.0006\n",
    "Processed batch 750/1875, Loss: 0.0062\n",
    "Processed batch 1000/1875, Loss: 0.0101\n",
    "Processed batch 1250/1875, Loss: 0.0044\n",
    "Processed batch 1500/1875, Loss: 0.0190\n",
    "Processed batch 1750/1875, Loss: 0.0028\n",
    "Train Loss: 0.0353\n",
    "Epoch Training Time: 1040.06 seconds\n",
    "Validation Loss: 0.2013, Validation Accuracy: 94.62%\n",
    "Full Epoch Time: 1075.99 seconds\n",
    "Best validation accuracy of 94.62 achieved at epoch 4"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
