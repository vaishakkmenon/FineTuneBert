{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13af748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical CPUs: 128\n",
      "Number of logical CPUs: 128\n",
      "Number of GPUs: 2\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "\tMemory Total: 81920.0 MB\n",
      "\tMemory Used: 7.0 MB\n",
      "\tMemory Free: 81042.0 MB\n",
      "\tGPU Utilization: 0.0%\n",
      "\tGPU Temperature: 31.0 °C\n",
      "GPU 2: NVIDIA A100 80GB PCIe\n",
      "\tMemory Total: 81920.0 MB\n",
      "\tMemory Used: 7.0 MB\n",
      "\tMemory Free: 81042.0 MB\n",
      "\tGPU Utilization: 0.0%\n",
      "\tGPU Temperature: 27.0 °C\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# GPU and CPU Check Code\n",
    "# KEEP AT THE TOP\n",
    "############################\n",
    "\n",
    "# !pip install psutil\n",
    "# !pip install gputil\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForMaskedLM, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "import psutil\n",
    "\n",
    "# Get the number of CPUs\n",
    "num_cpus = psutil.cpu_count(logical=False)  # physical cores\n",
    "num_logical_cpus = psutil.cpu_count(logical=True)  # logical cores\n",
    "\n",
    "print(f\"Number of physical CPUs: {num_cpus}\")\n",
    "print(f\"Number of logical CPUs: {num_logical_cpus}\")\n",
    "\n",
    "try:\n",
    "    import GPUtil\n",
    "\n",
    "    # Get the number of available GPUs\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    num_gpus = len(gpus)\n",
    "\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i + 1}: {gpu.name}\")\n",
    "        print(f\"\\tMemory Total: {gpu.memoryTotal} MB\")\n",
    "        print(f\"\\tMemory Used: {gpu.memoryUsed} MB\")\n",
    "        print(f\"\\tMemory Free: {gpu.memoryFree} MB\")\n",
    "        print(f\"\\tGPU Utilization: {gpu.load * 100}%\")\n",
    "        print(f\"\\tGPU Temperature: {gpu.temperature} °C\")\n",
    "except ImportError:\n",
    "    print(\"GPUtil library not found. Cannot check GPU information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "720e1b13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadError",
     "evalue": "file could not be opened successfully:\n- method gz: ReadError('invalid header')\n- method bz2: ReadError('not a bzip2 file')\n- method xz: ReadError('not an lzma file')\n- method tar: ReadError('invalid header')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m tar_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/qrels.trec8.qa.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m extract_to \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mextract_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mextract_all_files\u001b[0;34m(tar_file_path, extract_to)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_all_files\u001b[39m(tar_file_path, extract_to):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[1;32m      7\u001b[0m         tar\u001b[38;5;241m.\u001b[39mextractall(extract_to)\n",
      "File \u001b[0;32m/data/user/vmenon19/Conda_Env/nlp2023v2/lib/python3.10/tarfile.py:1629\u001b[0m, in \u001b[0;36mTarFile.open\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m     error_msgs_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m-> 1629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile could not be opened successfully:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_msgs_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1632\u001b[0m     filemode, comptype \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mReadError\u001b[0m: file could not be opened successfully:\n- method gz: ReadError('invalid header')\n- method bz2: ReadError('not a bzip2 file')\n- method xz: ReadError('not an lzma file')\n- method tar: ReadError('invalid header')"
     ]
    }
   ],
   "source": [
    "# # USE ONLY TO EXTRACT FILES FROM TAR FILES\n",
    "\n",
    "# import tarfile\n",
    "\n",
    "# def extract_all_files(tar_file_path, extract_to):\n",
    "#     with tarfile.open(tar_file_path, 'r') as tar:\n",
    "#         tar.extractall(extract_to)\n",
    "\n",
    "# # Example usage\n",
    "# tar_file_path = 'datasets/qrels.trec8.qa.gz'\n",
    "# extract_to = 'datasets/'\n",
    "# extract_all_files(tar_file_path, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c6b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:23<00:00, 299.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to read texts from files within a folder\n",
    "def read_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read().strip())\n",
    "    return texts\n",
    "\n",
    "# Function to preprocess texts\n",
    "def preprocess(texts, tokenizer, max_length=512):\n",
    "    all_input_ids = []\n",
    "    for text in tqdm(texts):\n",
    "        # Tokenize using the provided tokenizer\n",
    "        tokenized = tokenizer.encode_plus(text, max_length=max_length, truncation=True, padding='max_length')\n",
    "        input_ids = tokenized['input_ids']\n",
    "        all_input_ids.append(input_ids)\n",
    "\n",
    "    return all_input_ids\n",
    "\n",
    "# Paths to the directories within aclImdb folder\n",
    "aclImdb_folder = \"datasets/aclImdb\"\n",
    "train_pos_path = os.path.join(aclImdb_folder, 'train', 'pos')\n",
    "train_neg_path = os.path.join(aclImdb_folder, 'train', 'neg')\n",
    "\n",
    "# Initialize the BERT-base-uncased tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Read and preprocess the texts from positive and negative folders\n",
    "train_pos_texts = read_texts_from_folder(train_pos_path)\n",
    "train_neg_texts = read_texts_from_folder(train_neg_path)\n",
    "train_texts = train_pos_texts + train_neg_texts\n",
    "\n",
    "processed = preprocess(train_texts, tokenizer)\n",
    "\n",
    "print(\"EDITING FILE\")\n",
    "\n",
    "pretrain_file_path = \"pretraining_text.txt\"\n",
    "with open(pretrain_file_path, 'w', encoding='utf-8') as pretrain_file:\n",
    "    for text_ids in processed:\n",
    "        text = tokenizer.decode(text_ids, skip_special_tokens=True)\n",
    "        pretrain_file.write(text + '\\n')\n",
    "\n",
    "print(\"DONE EDITING\")\n",
    "        \n",
    "# Create a dataset for pre-training\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=pretrain_file_path,\n",
    "    block_size=512  # Adjust the block size as per your sequence length\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Probability of masking tokens\n",
    ")\n",
    "\n",
    "# Initialize the BERT masked language model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrained_bert\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=10000\n",
    ")\n",
    "\n",
    "# Create Trainer instance for pre-training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"STARTED TRAINING\")\n",
    "\n",
    "# Within the training loop\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # Perform training steps\n",
    "    trainer.train()\n",
    "\n",
    "    # Print training progress\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}/{total_steps}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"TRAINING DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc594f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "# Regex to extract category, subcategory, and question\n",
    "regex = r\"([\\w]+):([\\w]+) (.*)\"\n",
    "\n",
    "def read_questions(filepath):\n",
    "    questions = []\n",
    "    categories = set()\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            match = re.search(regex, line)\n",
    "            if match:\n",
    "                category = match.group(1)\n",
    "                subcategory = match.group(2)\n",
    "                question = match.group(3)\n",
    "                questions.append((category, subcategory, question))\n",
    "                categories.add(category)\n",
    "    return questions, list(categories)\n",
    "\n",
    "def preprocess(questions, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for cat, subcat, q in questions:\n",
    "        encoded = tokenizer(q, truncation=True, padding='max_length')\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Paths\n",
    "dataset_dir = \"./datasets\"\n",
    "filename = \"TREC_test.txt\"\n",
    "filepath = os.path.join(dataset_dir, filename)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Parse and tokenize\n",
    "questions, categories = read_questions(filepath)\n",
    "input_ids, attention_masks = preprocess(questions, tokenizer)\n",
    "\n",
    "# Assign labels dynamically to categories\n",
    "label_map = {cat: label for label, cat in enumerate(categories)}\n",
    "\n",
    "# Prepare labels based on the assigned labels for categories\n",
    "labels = [label_map[cat] for cat, _, _ in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33fc79d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datasets/yahoo_answers_csv/classes.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load class labels\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/datasets/yahoo_answers_csv/classes.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     categories \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f] \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n",
      "File \u001b[0;32m/data/user/vmenon19/Conda_Env/nlp2023v2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/yahoo_answers_csv/classes.txt'"
     ]
    }
   ],
   "source": [
    "# YAHOO DATASET READING\n",
    "\n",
    "import csv\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load class labels\n",
    "with open('datasets/yahoo_answers_csv/classes.txt') as f:\n",
    "    categories = [line.strip() for line in f] \n",
    "\n",
    "# Load dataset\n",
    "texts = [] \n",
    "labels = []\n",
    "with open('datasets/yahoo_answers_csv/train.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) # Skip header\n",
    "    for row in reader:\n",
    "        label = int(row[0]) - 1 # Class index starts from 1\n",
    "        text = \"{} {}\".format(row[1], row[2]) # Title + Content\n",
    "        text = re.sub(r'\\\\\"', '\"', text) # Unescape quotes\n",
    "        text = re.sub(r'\\\\n', '', text)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "        \n",
    "# Tokenize        \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')   \n",
    "input_ids = []\n",
    "attn_masks = [] \n",
    "\n",
    "for text in texts:\n",
    "    encoded = tokenizer(text, truncation=True, padding='max_length')\n",
    "    input_ids.append(encoded['input_ids'])\n",
    "    attn_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "print(f\"{len(input_ids)} examples tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445e4357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/local/ipykernel_68785/3863897988.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids).to(device)\n",
      "/scratch/local/ipykernel_68785/3863897988.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks = torch.tensor(attention_masks).to(device)\n",
      "/scratch/local/ipykernel_68785/3863897988.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4d2ecffc9c4cd891d6b4042f40e7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.668364830315113\n",
      "Training loss: 1.3145403265953064\n",
      "Training loss: 1.0003667697310448\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model architecture\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(categories)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "input_ids = torch.tensor(input_ids).to(device)\n",
    "attention_masks = torch.tensor(attention_masks).to(device)\n",
    "labels = torch.tensor(labels).to(device)\n",
    "\n",
    "# Dataloaders\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=SequentialSampler(dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 3  # Define the number of epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1, epochs + 1), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = loss_fct(logits, batch[2])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "    print(f'Training loss: {loss_train / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66826212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
